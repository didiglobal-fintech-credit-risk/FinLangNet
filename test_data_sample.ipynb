{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4be97668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf8')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data as data_\n",
    "import torch\n",
    "import argparse\n",
    "from torch.utils.data import IterableDataset\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import io\n",
    "from random import shuffle\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.nn import Linear, ReLU, Sigmoid, Module, BCELoss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau,CosineAnnealingLR,StepLR\n",
    "from torch.nn.utils import weight_norm\n",
    "from math import sqrt\n",
    "import torch.nn.functional as F\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4572315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inquiry_amt': Index([1000.0, 250000000.0], dtype='float64'), 'inquiry_date_ago': Index([58.0, 119.0, 190.0, 272.0, 365.0, 468.0, 588.0, 730.0], dtype='float64')}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('./data_bins/bin_edges_dict_dz_numeric_fusing_v5_8bin.pkl', 'rb') as handle:\n",
    "    bin_edges_dict_dz_numeric = pickle.load(handle)\n",
    "\n",
    "    \n",
    "with open('./data_bins/bin_edges_dict_inquery_numeric_fusing_v5_8bin.pkl', 'rb') as handle:\n",
    "    bin_edges_dict_inquery_numeric = pickle.load(handle)\n",
    "print(bin_edges_dict_inquery_numeric)\n",
    "   \n",
    "    \n",
    "with open('./data_bins/bin_edges_dict_creditos_numeric_fusing_v5_8bin.pkl', 'rb') as handle:\n",
    "    bin_edges_dict_creditos_numeric = pickle.load(handle)    \n",
    "    \n",
    "with open('./data_bins/bin_edges_dict_person_numeric_fusing_v5_8bin.pkl', 'rb') as handle:\n",
    "    bin_edges_dict_person_numeric = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d99217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"A simple tokenizer to convert between tokens and indices.\"\"\"\n",
    "    \n",
    "    def __init__(self, token_to_index=None, index_to_token=None):\n",
    "        \"\"\"\n",
    "        Initializes the tokenizer.\n",
    "\n",
    "        Args:\n",
    "            token_to_index (dict, optional): A dictionary mapping tokens to their indices.\n",
    "            index_to_token (dict, optional): A dictionary mapping indices back to tokens.\n",
    "        \"\"\"\n",
    "        self.token_to_index = token_to_index or {}\n",
    "        self.index_to_token = index_to_token or {}\n",
    "        self.unk_token = \"<unk>\"  # The token representing unknown words.\n",
    "        self.unk_index = 0  # Default index for unknown tokens (`<unk>`).\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        Fits the tokenizer on the data to build the mapping dictionaries.\n",
    "\n",
    "        Args:\n",
    "            data (list): The dataset containing tokens to build the mappings from.\n",
    "        \"\"\"\n",
    "        unique_tokens = sorted(set(data))  # Ensure consistent order.\n",
    "        self.token_to_index = {token: index for index, token in enumerate(unique_tokens, start=1)}\n",
    "        self.index_to_token = {index: token for token, index in self.token_to_index.items()}\n",
    "        self.unk_index = 0  # Reset the index for `<unk>` in case it changes.\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        Transforms the data into indices using the learned token-to-index map.\n",
    "\n",
    "        Args:\n",
    "            data (list): The data to be transformed into indices.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of indices corresponding to the tokens in `data`.\n",
    "        \"\"\"\n",
    "        return [self.token_to_index.get(item, self.unk_index) for item in data]\n",
    "\n",
    "    def reverse_transform(self, data):\n",
    "        \"\"\"\n",
    "        Converts indices back into tokens using the index-to-token map.\n",
    "\n",
    "        Args:\n",
    "            data (list): The indices to be converted back into tokens.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of tokens corresponding to the indices in `data`.\n",
    "        \"\"\"\n",
    "        return [self.index_to_token.get(index, self.unk_token) for index in data]\n",
    "\n",
    "\n",
    "# Define a list of inquiry types.\n",
    "inquiry_type_list = [\n",
    "    'F', 'TC', 'NC', 'M', 'PP', 'R', 'Q', 'LC', 'CA', 'OT', 'AM', 'PG', 'E', 'PN', 'AE', 'P', 'PM', 'AR', \n",
    "    'NG', 'A', 'FI', 'L', 'null', 'TG', 'FT', 'LR', 'CO', 'CF', 'EQ', 'VR', 'ED', 'AV', 'MC', 'BL', 'SH', 'HE'\n",
    "]\n",
    "\n",
    "# Create a tokenizer for inquiry types.\n",
    "inquiry_type_tokenizer = Tokenizer()\n",
    "inquiry_type_tokenizer.fit(inquiry_type_list)\n",
    "\n",
    "# Define a list of income categories.\n",
    "month_income_list = [\n",
    "    '5,000-10,000MXN', '10,000-20,000MXN', '3,000-5,000MXN', '20,000-50,000MXN', '1,000-3,000MXN', 'below 1,000MXN', 'above 50,000MXN'\n",
    "]\n",
    "\n",
    "# Create a tokenizer for monthly income categories.\n",
    "month_income_tokenizer = Tokenizer()\n",
    "month_income_tokenizer.fit(month_income_list)\n",
    "\n",
    "\n",
    "\n",
    "def map_values_to_bins(values_str, bin_edges):\n",
    "    values = np.array(list(map(float, values_str.split(','))))  # 将字符串值转换成浮点数列表\n",
    "    binned_values =  np.digitize(values, bins=bin_edges, right=True).tolist()\n",
    "    binned_values = [int(x) if not (x == float('inf') or x == float('-inf')) else 0 for x in binned_values]\n",
    "\n",
    "    return binned_values\n",
    "\n",
    "\n",
    "def map_values_to_bins_single(values_str, bin_edges):\n",
    "    values = np.array(list(map(float, values_str.split(','))))  # 将字符串值转换成浮点数列表\n",
    "    binned_values =  np.digitize(values, bins=bin_edges, right=True).tolist()[0]\n",
    "#     print(binned_values)\n",
    "    return binned_values\n",
    "\n",
    "\n",
    "class MyData(IterableDataset):\n",
    "    def __init__(self, file_paths, is_train=True, augmentation_prob=0.8,shuffle_files=True):\n",
    "        if shuffle_files:\n",
    "            shuffle(file_paths)\n",
    "        self.file_paths = file_paths\n",
    "        self.is_train = is_train\n",
    "        self.augmentation_prob = augmentation_prob\n",
    "        \n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        world_size = 1\n",
    "        rank = 0\n",
    "\n",
    "        worker_id = 0\n",
    "        mod = world_size\n",
    "        shift = rank\n",
    "\n",
    "        if worker_info:\n",
    "            num_workers = worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "            mod *= num_workers\n",
    "            shift = rank * num_workers + worker_id\n",
    "\n",
    "\n",
    "        line_iter = itertools.chain(*[io.open(f, encoding=\"utf-8\") for f in self.file_paths])\n",
    "        for i, line in enumerate(line_iter):\n",
    "            if (i + shift) % mod == 0:\n",
    "                return_line = self.process(line)\n",
    "                if return_line:\n",
    "                    yield return_line\n",
    "\n",
    "\n",
    "    def process(self, line):\n",
    "        text_data = line\n",
    "        dz_categorica_feature, dz_numeric_feature, person_feature , label, \\\n",
    "        len_dz, cfrnid, bv1_score,date_time_credit, inquery_feature, creditos_feature, len_inquery, len_creditos = process_one_sample(text_data)\n",
    "        dz_categorica_feature = torch.tensor(dz_categorica_feature)\n",
    "        dz_numeric_feature = torch.tensor(dz_numeric_feature)\n",
    "        person_feature = torch.tensor(person_feature)\n",
    "        label = torch.tensor(label)\n",
    "        if cfrnid != 360287970202376449:\n",
    "            return False\n",
    "        cfrnid = torch.tensor(cfrnid)\n",
    "        bv1_score = torch.tensor(bv1_score)\n",
    "        date_time_credit = torch.tensor(date_time_credit)\n",
    "        \n",
    "        inquery_feature = torch.tensor(inquery_feature)\n",
    "        creditos_feature = torch.tensor(creditos_feature)\n",
    "        \n",
    "        tuple_ = (dz_categorica_feature, dz_numeric_feature, person_feature,label, len_dz, cfrnid, bv1_score,date_time_credit, inquery_feature, creditos_feature, len_inquery, len_creditos)\n",
    "        return tuple_\n",
    "    \n",
    "    \n",
    "\n",
    "def process_2d_list(lst, k, pad_token=120):\n",
    "    result = []\n",
    "    for sub_lst in lst:\n",
    "        if len(sub_lst) < k:\n",
    "            sub_lst += [pad_token] * (k - len(sub_lst))\n",
    "            result.append(sub_lst)\n",
    "        else:\n",
    "            result.append(sub_lst[-k:])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f360df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b743e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71308265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_sample(features,max_length=120):\n",
    "    feature1, feature2, len1, label1, label2, label3, feature3, feature4, feature5, feature6, feature7, feature8 = features.split('\\t')\n",
    "    feature2 = feature2.strip('\\n')  \n",
    "    label = [int(label1),int(label2),int(label3)]  \n",
    "    feature1 = float(feature1)    \n",
    "    cls_token = 121 # Define class sequence feature token to represent one sentence\n",
    "    \n",
    "    # Example\n",
    "    feature1 = month_income_tokenizer.transform([feature1])[0]\n",
    "    feature2 = inquiry_type_tokenizer.transform([feature2])[0]\n",
    "    person_categorica_feature = [feature1]\n",
    "    person_numeric_feature = [feature2]\n",
    "    person_numeric_feature_discretized = [map_values_to_bins_single(data_str, bin_edges_dict_person_numeric[feature_name]) for feature_name, data_str in zip(list(bin_edges_dict_person_numeric.keys()), person_numeric_feature)]\n",
    "    person_feature = person_categorica_feature  + person_numeric_feature_discretized\n",
    "    dz_categorica_feature = [feature3]\n",
    "    dz_categorica_feature = [fe.split(',') for fe in dz_categorica_feature]\n",
    "    dz_numeric_feature = [feature4]\n",
    "    dz_numeric_feature = [map_values_to_bins(data_str, bin_edges_dict_dz_numeric[feature_name]) for feature_name, data_str in zip(list(bin_edges_dict_dz_numeric.keys()), dz_numeric_feature)]\n",
    " \n",
    "    max_length = 25\n",
    "    len_dz= len(dz_categorica_feature[0])\n",
    "    len_dz = len_dz if len_dz <=max_length else max_length\n",
    "    dz_categorica_feature = [[int(value) for value in row] for row in dz_categorica_feature]\n",
    "    dz_numeric_feature = [[float(value) if value != '' else 0.0 for value in row] for row in dz_numeric_feature]\n",
    "    dz_categorica_feature = process_2d_list(dz_categorica_feature,max_length)\n",
    "    # Adjust features to max_length and prepend class token\n",
    "    dz_categorica_feature = [[cls_token] + sublist for sublist in dz_categorica_feature]\n",
    "    dz_numeric_feature = process_2d_list(dz_numeric_feature,max_length)\n",
    "    dz_numeric_feature = [[cls_token] + sublist for sublist in dz_numeric_feature]\n",
    "    \n",
    "    print('dz_categorica_feature summary include ', len(dz_categorica_feature), 'sentences',  ' and dz_categorica_feature one sentence like : ', dz_categorica_feature[1])\n",
    "    print('dz_numeric_feature summary include : ', len(dz_numeric_feature), 'sentences', ' and dz_numeric_feature one sentence like : ', dz_numeric_feature[3])\n",
    "    print('inquery_feature summary include : ', len(inquery_feature), 'sentences', ' and inquery_feature one sentence like : ', inquery_feature[4])\n",
    "    print('creditos_feature summary include : ', len(creditos_feature), 'sentences', ' and creditos_feature one sentence like : ', creditos_feature[2])\n",
    "\n",
    "    return dz_categorica_feature, dz_numeric_feature, person_feature, label, len_dz\n",
    "   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "843e016a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz_categorica_feature summary include  4 sentences  and dz_categorica_feature one sentence like :  [121, 0, 0, 1, 1, 1, 1, 1, 1, 1, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120]\n",
      "dz_numeric_feature summary include :  237 sentences  and dz_numeric_feature one sentence like :  [121, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120]\n",
      "inquery_feature summary include :  5 sentences  and inquery_feature one sentence like :  [121, 7, 7, 7, 6, 6, 5, 4, 4, 3, 3, 2, 2, 2, 1, 1, 1, 0, 0, 0, 0, 0, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120]\n",
      "creditos_feature summary include :  29 sentences  and creditos_feature one sentence like :  [121, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch  # Assume that PyTorch is used for model evaluation\n",
    "\n",
    "def load_data_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Load and return data from a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    - dict: The data loaded from the JSON file as a dictionary.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def evaluate(input_data):\n",
    "    \"\"\"\n",
    "    Evaluate the model with features extracted from the input data.\n",
    "\n",
    "    Parameters:\n",
    "    - input_data (dict): The dictionary containing features and data to evaluate.\n",
    "\n",
    "    Raises:\n",
    "    - Exception: Throws an exception if necessary features are missing, or if there's an error during preprocessing or model prediction.\n",
    "\n",
    "    Notes:\n",
    "    - Assume process_one_sample and other functions are defined elsewhere and handle specific data preparation and model evaluation tasks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data_input = []\n",
    "        features_list = columns_num  # Assume columns_num is defined elsewhere and contains the list of required features\n",
    "        for feature in features_list:\n",
    "            data_input.append(input_data[feature])\n",
    "    except Exception as e:\n",
    "        raise Exception(\"Incomplete features for model input, please check your configuration!\") from e\n",
    "\n",
    "    try:\n",
    "        # Example function call - replace with actual preprocessing and evaluation\n",
    "        dz_categorica_feature, dz_numeric_feature, person_feature, label, len_dz, cfrnid, bv1_score, date_time_credit, inquery_feature, creditos_feature, len_inquery, len_creditos = process_one_sample(data_input)\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(\"Error during feature preprocessing!\") from e\n",
    "    \n",
    "    try:\n",
    "        val_preds = []\n",
    "        val_score = []\n",
    "        # Model evaluation logic goes here\n",
    "        with torch.no_grad():\n",
    "            pass  # Replace with actual evaluation logic\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(\"Anomaly during model prediction process!\") from e\n",
    "        \n",
    "file_path = 'path_to_your_file.json'  # Specify the path to your JSON file\n",
    "file_path = './data.json'\n",
    "# Load data from JSON file\n",
    "data_dict = load_data_from_json(file_path)\n",
    "\n",
    "# Evaluate the model with loaded data\n",
    "evaluate(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a01c170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c7944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
