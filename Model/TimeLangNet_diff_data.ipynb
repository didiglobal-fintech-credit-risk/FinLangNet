{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b56d532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import ipynbname\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sktime.datasets import load_from_tsfile_to_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffe04505",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mleiyu0210\u001b[0m (\u001b[33mtemporal_name\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/dataset-100116-1/leiyu/AAAI2025/wandb/run-20240801_003657-lbqvwh6d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/temporal_name/llm_graph_sequence/runs/lbqvwh6d' target=\"_blank\">TimeLangNet_diff_data</a></strong> to <a href='https://wandb.ai/temporal_name/llm_graph_sequence' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/temporal_name/llm_graph_sequence' target=\"_blank\">https://wandb.ai/temporal_name/llm_graph_sequence</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/temporal_name/llm_graph_sequence/runs/lbqvwh6d' target=\"_blank\">https://wandb.ai/temporal_name/llm_graph_sequence/runs/lbqvwh6d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/temporal_name/llm_graph_sequence/runs/lbqvwh6d?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ff81c4951b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the hypers for the model\n",
    "from argparse import Namespace\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "config = Namespace()\n",
    "\n",
    "config.batch_size = 2\n",
    "config.padding_idx = 99999\n",
    "config.defaut_max_size = 500\n",
    "# config.defaut_max_size_hash = 500\n",
    "\n",
    "config.ts_max_length = None\n",
    "\n",
    "config.with_feature_padding = True\n",
    "config.with_feature_prompt = True \n",
    "\n",
    "config.ts_feature_size = None\n",
    "config.ts_max_idx = None\n",
    "\n",
    "config.hidden_dim = 64\n",
    "config.with_prompt = True \n",
    "config.TimeLangNet_embedding_dim = 64\n",
    "config.TimeLangNet_layers = 2\n",
    "config.TimeLangNet_heads  = 2\n",
    "config.TimeLangNet_d_k = 4\n",
    "config.TimeLangNet_d_v = 4\n",
    "config.num_class = None\n",
    "\n",
    "config.epoch = 30\n",
    "# config.lr = 5e-4\n",
    "config.lr = 0.001\n",
    "config.l2_weight = 0.01\n",
    "\n",
    "config.ALPHA = 0.4\n",
    "config.BETA = 0.6\n",
    "config.GAMMA = 1\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffabefe9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ArticularyWordRecognition'], ['AtrialFibrillation'], ['BasicMotions'], ['Cricket'], ['DuckDuckGeese'], ['EigenWorms'], ['Epilepsy'], ['EthanolConcentration'], ['ERing'], ['FaceDetection'], ['FingerMovements'], ['HandMovementDirection'], ['Handwriting'], ['Heartbeat'], ['Libras'], ['LSST'], ['MotorImagery'], ['NATOPS'], ['PenDigits'], ['PEMS-SF'], ['PhonemeSpectra'], ['RacketSports'], ['SelfRegulationSCP1'], ['SelfRegulationSCP2'], ['StandWalkJump'], ['UWaveGestureLibrary']]\n",
      "====================================RUNNING=====================================\n",
      "----------------------------['EthanolConcentration']----------------------------\n",
      "Loading data...............................................................Done.\n"
     ]
    }
   ],
   "source": [
    "path=\"Multivariate_ts/\" #datasets path \n",
    "flist = pd.read_csv(\"MSTC_Data.csv\", header=None)\n",
    "flist = flist.to_numpy().tolist()\n",
    "\n",
    "print(flist)\n",
    "\n",
    "def readucr(filename):\n",
    "    data= load_from_tsfile_to_dataframe(filename)\n",
    "    return data\n",
    "dataset_names = [['EthanolConcentration']]\n",
    "\n",
    "def preprocess_data(x, y):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    x = x.to_numpy()\n",
    "    class_le = LabelEncoder()\n",
    "    y_encoded = class_le.fit_transform(y.copy())\n",
    "    channels = x.shape[1]\n",
    "    data_row = len(x[0][0])\n",
    "    x_processed = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_sample = np.concatenate(x[i], axis=0).reshape(channels, data_row, 1)\n",
    "        x_processed.append(x_sample)\n",
    "    \n",
    "    x_float = np.float32(np.array(x_processed)[:,:,:,0])\n",
    "    \n",
    "    return x_float, np.float32(y_encoded)\n",
    "\n",
    "results = pd.DataFrame(index = dataset_names,\n",
    "                       columns = [\"accuracy_mean\",\n",
    "                                  \"accuracy_standard_deviation\",\n",
    "                                  \"time_training_seconds\",\n",
    "                                  \"time_test_seconds\"],\n",
    "                       data = 0)\n",
    "results.index.name = \"dataset\"\n",
    "\n",
    "print(f\"RUNNING\".center(80, \"=\"))\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"{dataset_name}\".center(80, \"-\"))\n",
    "\n",
    "    # -- read data -------------------------------------------------------------\n",
    "\n",
    "    print(f\"Loading data\".ljust(80 - 5, \".\"), end = \"\", flush = True)\n",
    "    x_train, y_train = readucr(path + dataset_name[0] + '/' + dataset_name[0] + '_TRAIN.ts')\n",
    "    X_train, Y_train = preprocess_data(x_train, y_train)\n",
    "\n",
    "    x_test, y_test = readucr(path + dataset_name[0] + '/' + dataset_name[0] + '_TEST.ts')\n",
    "    X_test, Y_test = preprocess_data(x_test, y_test)\n",
    "    \n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df3251c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((261, 3, 1751), (261,), (263, 3, 1751), (263,), 3.0, 3.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape, max(Y_train), max(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d633ad43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1751, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, config.ts_feature_size, config.ts_max_length = X_train.shape\n",
    "config.num_class = int(max(Y_train) + 1)\n",
    "config.ts_feature_size, config.ts_max_length, config.num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "697619f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TimeLangNet_Encoder:\n",
    "    def __init__(self, bins='auto'):\n",
    "        self.bins = bins\n",
    "        self.bin_edges_ = None\n",
    "        self.train_type = 1\n",
    "        \n",
    "    def fit(self, X, feature_names):\n",
    "        batch_size, feature_dim, feature_length = X.shape\n",
    "        self.bin_edges_ = []\n",
    "\n",
    "        for i in range(feature_dim):\n",
    "            feature_data = X[:, i, :].flatten()\n",
    "\n",
    "            if isinstance(self.bins, str) and self.bins == 'auto':\n",
    "                bins = np.histogram_bin_edges(feature_data, bins='auto')\n",
    "            elif isinstance(self.bins, int):\n",
    "                bins = np.linspace(feature_data.min(), feature_data.max(), self.bins+1)\n",
    "            elif isinstance(self.bins, (np.ndarray, list, tuple)):\n",
    "                bins = np.array(self.bins)\n",
    "            elif isinstance(self.bins, dict):\n",
    "                for feature in feature_names:\n",
    "                    if feature in self.bins:\n",
    "                        bins = self.bins[feature]\n",
    "                        if not isinstance(bins, np.ndarray):\n",
    "                            bins = np.array(bins)\n",
    "                        self.bin_edges_.append(bins)\n",
    "                    else:\n",
    "                        raise ValueError(\"Bin edges are not provided for feature '%s'.\" % feature)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid bin definition.\")\n",
    "                \n",
    "            self.bin_edges_.append(bins)\n",
    " \n",
    "            \n",
    "    def process_add_prompt(self, input_list, k, ts_max_idxs):\n",
    "        result = []\n",
    "        \n",
    "        for sub_lst, max_idx in zip(input_list, ts_max_idxs):\n",
    "            if len(sub_lst) < k:\n",
    "                sub_lst += [max_idx - 1] * (k - len(sub_lst))\n",
    "                \n",
    "                if config.with_feature_prompt:\n",
    "                    result.append([max_idx - 2] + sub_lst)\n",
    "                else:\n",
    "                    result.append(sub_lst)\n",
    "            else:\n",
    "                if config.with_feature_prompt:\n",
    "                    result.append([max_idx - 2] + sub_lst[-k:])\n",
    "                else:\n",
    "                    result.append(sub_lst[-k:])\n",
    "        return result\n",
    "\n",
    "    def transform(self, X, ts_max_idxs, max_length):\n",
    "        if self.bin_edges_ is None:\n",
    "            raise ValueError(\"The fit method should be called before transform.\")\n",
    "\n",
    "        batch_size, feature_dim, length = X.shape\n",
    "        full_binned_features = []\n",
    "\n",
    "        if ts_max_idxs is None:\n",
    "            ts_max_idxs = [len(bins) + 1 for bins in self.bin_edges_]\n",
    "            if config.with_feature_padding:\n",
    "                ts_max_idxs = [max_idx + 1 for max_idx in ts_max_idxs]\n",
    "            if config.with_feature_prompt:\n",
    "                ts_max_idxs = [max_idx + 1 for max_idx in ts_max_idxs]\n",
    "                \n",
    "            config.ts_max_idxs = ts_max_idxs        \n",
    "        \n",
    "        if self.train_type:\n",
    "            binned_features_by_bs = []\n",
    "\n",
    "            batch_real_lengths = []\n",
    "            for j in range(batch_size):\n",
    "                binned_features = []\n",
    "                real_length = length\n",
    "                for i in range(feature_dim):\n",
    "                    bins = self.bin_edges_[i]\n",
    "                    binned_feature_transformer = [np.digitize(x, bins, right=True).tolist() for x in X[j:j+1, i]]   \n",
    "                    \n",
    "                    real_length = max(real_length, len(binned_feature_transformer))\n",
    "                    \n",
    "                    binned_feature_with_prompt = self.process_add_prompt(binned_feature_transformer, max_length, ts_max_idxs)\n",
    "                    binned_features.append(binned_feature_with_prompt)\n",
    "                    \n",
    "                binned_features_by_bs.append(binned_features)\n",
    "                batch_real_lengths.append(real_length)\n",
    "                \n",
    "            binned_features_array = np.squeeze(np.array(binned_features_by_bs))\n",
    "            \n",
    "            if batch_size == 1:\n",
    "                binned_features_array = np.expand_dims(binned_features_array, axis=0)\n",
    "                \n",
    "            return binned_features_array, batch_real_lengths\n",
    "    \n",
    "\n",
    "    def fit_transform(self, X, feature_names=None, ts_max_idxs=None, max_length=None):\n",
    "        self.fit(X, feature_names)\n",
    "        return self.transform(X, ts_max_idxs, max_length)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b47a3ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 46, 46]\n",
      "[46, 46, 46]\n"
     ]
    }
   ],
   "source": [
    "encoder_auto = TimeLangNet_Encoder(bins=42)\n",
    "X_train, X_trian_length = encoder_auto.fit_transform(X_train, ts_max_idxs=config.ts_max_idx, max_length=config.ts_max_length)\n",
    "print(config.ts_max_idxs)\n",
    "X_test, X_test_length = encoder_auto.fit_transform(X_test, ts_max_idxs=config.ts_max_idx, max_length=config.ts_max_length)\n",
    "print(config.ts_max_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7556e6b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([2, 3, 1752])\n",
      "tensor([[[44.,  2.,  2.,  ...,  2.,  2.,  2.],\n",
      "         [44.,  2.,  2.,  ...,  2.,  2.,  2.],\n",
      "         [44.,  2.,  2.,  ...,  2.,  2.,  2.]],\n",
      "\n",
      "        [[44.,  2.,  2.,  ...,  2.,  2.,  2.],\n",
      "         [44.,  2.,  2.,  ...,  2.,  2.,  2.],\n",
      "         [44.,  2.,  2.,  ...,  2.,  2.,  2.]]])\n",
      "tensor([1751, 1751])\n",
      "label: tensor([0, 0])\n",
      "\n",
      "\n",
      "1\n",
      "torch.Size([2, 3, 1752])\n",
      "tensor([[[44.,  2.,  2.,  ...,  2.,  2.,  2.],\n",
      "         [44.,  2.,  2.,  ...,  2.,  2.,  2.],\n",
      "         [44.,  2.,  2.,  ...,  2.,  2.,  2.]],\n",
      "\n",
      "        [[44.,  2.,  2.,  ...,  2.,  2.,  2.],\n",
      "         [44.,  2.,  2.,  ...,  2.,  2.,  2.],\n",
      "         [44.,  2.,  2.,  ...,  2.,  2.,  2.]]])\n",
      "tensor([1751, 1751])\n",
      "label: tensor([0, 0])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def create_dataloader(X, Y, lengths, batch_size=16, shuffle=True):\n",
    "\n",
    "    tensor_x = torch.Tensor(X)  \n",
    "    tensor_y = torch.Tensor(Y).long()  \n",
    "    tensor_lengths = torch.Tensor(lengths).long()  \n",
    "    dataset = TensorDataset(tensor_x, tensor_y, tensor_lengths)\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = create_dataloader(X_train, Y_train, X_trian_length, batch_size=config.batch_size)\n",
    "test_dataloader = create_dataloader(X_test, Y_test, X_test_length, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "    \n",
    "cnt = 0\n",
    "for step, data in enumerate(test_dataloader):\n",
    "    if step > 1: break \n",
    "    print(step)\n",
    "    features , label, length= data\n",
    "    print(features.shape)\n",
    "    print(features)\n",
    "    print(length)\n",
    "    print('label:',label) \n",
    "    print(\"\\n\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "830d827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "import math\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "from random import * \n",
    "from torch.autograd import Variable\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "################### Utils     #########################\n",
    "class TriangularCausalMask():\n",
    "    def __init__(self, B, L, device=\"cpu\"):\n",
    "        mask_shape = [B, 1, L, L]\n",
    "        with torch.no_grad():\n",
    "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n",
    "\n",
    "    \n",
    "#####################   Embedding layers ###########################\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, max_len, d_model, dropout = 0.05):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "  \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        # print(self.pe[:, :x.size(1)].shape)\n",
    "        x = x + Variable(self.pe[:, :x.size(1)],  requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "    \n",
    "class Fusion(nn.Module):\n",
    "    def __init__(self, input_size, out=1, dropout=0.2):\n",
    "        super(Fusion, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, input_size)\n",
    "        self.linear2 = nn.Linear(input_size, out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init.xavier_normal_(self.linear1.weight)\n",
    "        init.xavier_normal_(self.linear2.weight)\n",
    "\n",
    "    def forward(self, input_embeddings):\n",
    "        emb = torch.stack(input_embeddings)\n",
    "        emb_score = F.softmax(self.linear2(torch.tanh(self.linear1(emb))), dim=0)\n",
    "        emb_score = self.dropout(emb_score)\n",
    "        out = torch.sum(emb_score * emb, dim=0)\n",
    "        return out\n",
    "\n",
    "class FeatureEmbedding(nn.Module):\n",
    "    def __init__(self, \n",
    "                 feature_size,\n",
    "                 feature_max_idxs,\n",
    "                 d_model,\n",
    "                 max_len=512,\n",
    "                 with_pos = False,\n",
    "                 with_prompt = True):\n",
    "        super(FeatureEmbedding, self).__init__()\n",
    "        \n",
    "        self.feature_size = feature_size\n",
    "        self.feature_max_idxs = feature_max_idxs\n",
    "        self.d_model = d_model\n",
    "        self.with_pos = with_pos \n",
    "        self.with_prompt = with_prompt\n",
    "        \n",
    "        self.embedding_layers = nn.ModuleList([nn.Embedding(num_embeddings=self.feature_max_idxs[i], embedding_dim=self.d_model, padding_idx = self.feature_max_idxs[i] - 1) for i in range(feature_size)])\n",
    "        if self.with_pos:\n",
    "            self.pos_embedding = PositionalEncoding(max_len, d_model)  # position embedding\n",
    "        self.prompt_token_embedding = nn.Parameter(torch.zeros(1, 1, self.d_model))\n",
    "        self.feature_weight = nn.Parameter(torch.zeros(self.feature_size))\n",
    "        self.fusion_layer = Fusion(input_size = self.d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def get_feature_embeding_with_sum(self, x):\n",
    "        \n",
    "        features_embeddings = []\n",
    "        for i in range(self.feature_size):\n",
    "            embedded_feature = self.embedding_layers[i](x[:, i, :].long()) # [bsz, max_len] -> [bsz, max_len, hidden_dim]\n",
    "            features_embeddings.append(embedded_feature)\n",
    "        features_embeddings = torch.stack(features_embeddings)\n",
    "        features_embedding = torch.sum(features_embeddings, dim=0)  #len*emb_dim\n",
    "#         features_embedding = self.fusion_layer(features_embeddings)\n",
    "        \n",
    "        return features_embedding\n",
    "    \n",
    "    def get_feature_embedding_with_add(self, x):\n",
    "        bsz, fsz, seq_len = x.size()\n",
    "            \n",
    "        # feature embeddings\n",
    "        features_embedding = torch.zeros(bsz, seq_len, self.d_model).to(device)\n",
    "        \n",
    "        for i in range(self.feature_size):\n",
    "            \n",
    "            embedded_feature = self.embedding_layers[i](x[:, i, :].long()) # [bsz, max_len] -> [bsz, max_len, hidden_dim]\n",
    "            \n",
    "            features_embedding += self.feature_weight[i] * embedded_feature\n",
    "        \n",
    "        return features_embedding\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x : [bsz, feature_size, max_len]\n",
    "\n",
    "        x_embedding = self.get_feature_embedding_with_add(x)\n",
    "        \n",
    "        if self.with_prompt:\n",
    "            prompt_token = self.prompt_token_embedding.expand(x.size(0), -1, -1)\n",
    "            x_embedding = torch.cat([prompt_token, x_embedding], dim=1) # [bsz, max_len, dim]\n",
    "        \n",
    "        if self.with_pos:\n",
    "            x_embedding = self.pos_embedding(x_embedding)\n",
    "\n",
    "        return self.norm(x_embedding)\n",
    "\n",
    "###################### main model #####################################\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None, tau=None, delta=None):\n",
    "        # x [B, L, D]\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for i, (attn_layer, conv_layer) in enumerate(zip(self.attn_layers, self.conv_layers)):\n",
    "                delta = delta if i == 0 else None\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n",
    "                attns.append(attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, attns\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None, tau=None, delta=None):\n",
    "        new_x, attn = self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask,\n",
    "            tau=tau, delta=delta\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "\n",
    "        y = x = self.norm1(x)\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "\n",
    "        return self.norm2(x + y), attn\n",
    "    \n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads, d_keys=None,\n",
    "                 d_values=None):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "\n",
    "        self.inner_attention = attention\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out, attn = self.inner_attention(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            attn_mask,\n",
    "            tau=tau,\n",
    "            delta=delta\n",
    "        )\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out), attn\n",
    "\n",
    "class FullAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_heads=8,\n",
    "                 scale=None, \n",
    "                 attention_dropout=0.1, \n",
    "                 output_attention=False, \n",
    "                 future_mask_flag = False,\n",
    "                 ):\n",
    "        super(FullAttention, self).__init__()\n",
    "        \n",
    "        self.scale = scale\n",
    "        self.future_mask_flag = future_mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "\n",
    "\n",
    "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys) / (sqrt(E) + 1e-6)\n",
    "\n",
    "        if self.future_mask_flag:\n",
    "            future_mask = TriangularCausalMask(B, L, device=queries.device)\n",
    "            batch_size, len_mask = future_mask.size()\n",
    "            future_mask = future_mask.view(batch_size, 1, len_mask, 1)\n",
    "            future_mask = future_mask.expand(-1, self.n_heads, -1, -1)\n",
    "            scores.masked_fill_(future_mask, -1e10)\n",
    "        if attn_mask != None:\n",
    "            #             attn_mask = attn_mask.unsqueeze(1).expand(-1, L, -1).bool()\n",
    "            #             attn_mask = attn_mask.unsqueeze(1).expand(-1, self.n_heads, -1, -1)\n",
    "            scores.masked_fill_(attn_mask, -1e10)\n",
    "\n",
    "        A = self.dropout(torch.softmax(scores, dim=-1))\n",
    "        V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return V.contiguous(), A\n",
    "        else:\n",
    "            return V.contiguous(), None\n",
    "\n",
    "class TimeLangNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 feature_size,\n",
    "                 feature_max_idxs,\n",
    "                 num_class,\n",
    "                 max_len = 512,\n",
    "                 n_layers = 6,  # number of Encoder of Encoder Layer\n",
    "                 n_heads = 12,  # number of heads in Multi-Head Attention\n",
    "                 d_model = 768, # Embedding Size\n",
    "                 d_ff = 3072,   # 4*d_model, FeedForward dimension\n",
    "                 d_k = 64,      # dimension of K(=Q), V\n",
    "                 d_v = 64,       # dimension of K(=Q), V\n",
    "                 output_dim = 256,\n",
    "                 with_prompt = True,\n",
    "                 future_mask_flag = False,\n",
    "                 ):\n",
    "        super(TimeLangNet, self).__init__()\n",
    "            \n",
    "        # hypers for the model\n",
    "        self.feature_size, = feature_size,\n",
    "        self.feature_max_idxs = feature_max_idxs\n",
    "        self.max_len = max_len\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.hidden_dim = 128\n",
    "        self.output_dim = output_dim \n",
    "        self.num_classes = num_class \n",
    "        \n",
    "        # flags  \n",
    "        self.future_mask_flag = future_mask_flag\n",
    "        self.with_prompt = with_prompt\n",
    "                                             \n",
    "        # modules for the model\n",
    "        self.embedding = FeatureEmbedding(feature_size = feature_size, \n",
    "                                          feature_max_idxs = feature_max_idxs,  \n",
    "                                          d_model = d_model, \n",
    "                                          max_len = max_len, \n",
    "                                          with_prompt = with_prompt) \n",
    "\n",
    "        self.dropout =  nn.Dropout(0.1)\n",
    "#         self.encoder = Encoder(d_model= d_model, d_keys = d_k, d_values = d_v, n_heads=n_heads, n_layers = n_layers,d_ff=d_ff,)\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(n_heads = self.n_heads,\n",
    "                                      output_attention = True, \n",
    "                                      future_mask_flag = False, \n",
    "                                      ), \n",
    "                        d_model = self.d_model, \n",
    "                        n_heads = self.n_heads,\n",
    "                        d_keys = self.d_k,\n",
    "                        d_values = self.d_v,\n",
    "                    ),\n",
    "                    d_model = self.d_model,\n",
    "                    d_ff = self.d_ff\n",
    "                ) for l in range(self.n_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(self.d_model)\n",
    "        )\n",
    "        self.out_linear = nn.Linear(self.d_model, output_dim)\n",
    "        \n",
    "        self.act = F.gelu\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(output_dim, num_class)\n",
    "        \n",
    "    def forward(self, input_features, valid_len):\n",
    "        \n",
    "        # hypers \n",
    "        bsz, fsz, max_len = input_features.size()\n",
    "        padding_mask = self.get_padding_mask(input_features) # [bsz, max_len]\n",
    "        embedded_features = self.embedding(input_features)  \n",
    "        transformer_output, _ = self.encoder(embedded_features, attn_mask=padding_mask) # BS*length*hidden_dim\n",
    "        transformer_output = self.act(transformer_output)\n",
    "        transformer_output = self.dropout(transformer_output)\n",
    "        output_represenetations = self.out_linear(transformer_output.mean(1))\n",
    "        \n",
    "        output = self.fc(output_represenetations)\n",
    "        \n",
    "#         return nn.functional.softmax(output, dim=1)\n",
    "        return output\n",
    "                                             \n",
    "    \n",
    "    def get_padding_mask(self, input_features):\n",
    "            bsz, fsz, max_len = input_features.size()\n",
    "#             max_len = max_len + 1 if self.with_prompt else max_len\n",
    "            \n",
    "            padding_mask = (input_features.sum(1)) == (sum(self.feature_max_idxs) - fsz) # [bsz, max_len]\n",
    "            if self.with_prompt:\n",
    "                padding_mask = torch.cat([torch.zeros(bsz,1).cuda(), padding_mask], dim=1) # [bsz, max_len +1 ]\n",
    "                padding_mask = padding_mask.bool().unsqueeze(1).repeat(1, max_len + 1, 1)\n",
    "            else:\n",
    "                padding_mask = padding_mask.bool().unsqueeze(1).repeat(1, max_len, 1)\n",
    "            padding_mask = padding_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
    "\n",
    "            \n",
    "            return padding_mask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fff4a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, recall_score, f1_score, precision_recall_curve, average_precision_score\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.nn import Linear, ReLU, Sigmoid, Module, BCELoss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau,CosineAnnealingLR,StepLR\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class MyModel_FinLangNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 ts_feature_size,\n",
    "                 ts_max_idxs,\n",
    "                 num_class,\n",
    "                 d_model=64,\n",
    "                 n_layers=4,\n",
    "                 n_heads=8,\n",
    "                 d_ff=64*4,\n",
    "                 d_k=8,\n",
    "                 d_v=8,\n",
    "                 hidden_dim=64,\n",
    "                ):\n",
    "        super(MyModel_FinLangNet, self).__init__()\n",
    "        \n",
    "        # hypers \n",
    "        self.ts_len = ts_feature_size\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.embedding_dim = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.hidden_dim = 64\n",
    "        self.ts_max_idxs = ts_max_idxs\n",
    "        self.num_class = num_class\n",
    "        # hyper functions for the model\n",
    "        self.act = F.gelu\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid 激活函数\n",
    "        # 时序特征处理 \n",
    "        # category_feature\n",
    "        # hyper parameters for the model        \n",
    "        self.TimeLangNet = TimeLangNet(self.ts_len,  \n",
    "                 feature_max_idxs = ts_max_idxs,\n",
    "                 num_class = self.num_class ,\n",
    "                 n_layers =  self.n_layers  ,  # number of Encoder of Encoder Layer\n",
    "                 n_heads  =  self.n_heads   ,  # number of heads in Multi-Head Attention\n",
    "                 d_model  =  self.d_model   , # Embedding Size\n",
    "                 d_ff     =  self.d_ff      ,   # 4*d_model, FeedForward dimension\n",
    "                 d_k      =  self.d_k       ,      # dimension of K(=Q), V\n",
    "                 d_v      =  self.d_v       ,      # dimension of K(=Q), V\n",
    "                 output_dim = 256, \n",
    "        )\n",
    "    \n",
    "    def forward(self, \n",
    "                ts_feature, \n",
    "                len_ts,\n",
    "                ):\n",
    "        \n",
    "        # 时序特征处理 \n",
    "        ts_feature_input =  ts_feature #BS*dim*length\n",
    "        ts_out = self.TimeLangNet(ts_feature_input, len_ts)\n",
    "        \n",
    "        return ts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1415ca6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel_FinLangNet(\n",
      "  (sigmoid): Sigmoid()\n",
      "  (TimeLangNet): TimeLangNet(\n",
      "    (embedding): FeatureEmbedding(\n",
      "      (embedding_layers): ModuleList(\n",
      "        (0): Embedding(46, 64, padding_idx=45)\n",
      "        (1): Embedding(46, 64, padding_idx=45)\n",
      "        (2): Embedding(46, 64, padding_idx=45)\n",
      "      )\n",
      "      (fusion_layer): Fusion(\n",
      "        (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (linear2): Linear(in_features=64, out_features=1, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (encoder): Encoder(\n",
      "      (attn_layers): ModuleList(\n",
      "        (0): EncoderLayer(\n",
      "          (attention): AttentionLayer(\n",
      "            (inner_attention): FullAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (query_projection): Linear(in_features=64, out_features=8, bias=True)\n",
      "            (key_projection): Linear(in_features=64, out_features=8, bias=True)\n",
      "            (value_projection): Linear(in_features=64, out_features=8, bias=True)\n",
      "            (out_projection): Linear(in_features=8, out_features=64, bias=True)\n",
      "          )\n",
      "          (conv1): Conv1d(64, 256, kernel_size=(1,), stride=(1,))\n",
      "          (conv2): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): EncoderLayer(\n",
      "          (attention): AttentionLayer(\n",
      "            (inner_attention): FullAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (query_projection): Linear(in_features=64, out_features=8, bias=True)\n",
      "            (key_projection): Linear(in_features=64, out_features=8, bias=True)\n",
      "            (value_projection): Linear(in_features=64, out_features=8, bias=True)\n",
      "            (out_projection): Linear(in_features=8, out_features=64, bias=True)\n",
      "          )\n",
      "          (conv1): Conv1d(64, 256, kernel_size=(1,), stride=(1,))\n",
      "          (conv2): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (out_linear): Linear(in_features=64, out_features=256, bias=True)\n",
      "    (fc): Linear(in_features=256, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "{'Total': 102008, 'Trainable': 102008}\n"
     ]
    }
   ],
   "source": [
    "# 创建模型实例\n",
    "model = MyModel_FinLangNet(\n",
    "    ts_feature_size = config.ts_feature_size,\n",
    "    ts_max_idxs = config.ts_max_idxs,\n",
    "    num_class = config.num_class,\n",
    "    d_model = config.TimeLangNet_embedding_dim,\n",
    "    n_layers = config.TimeLangNet_layers,\n",
    "    n_heads = config.TimeLangNet_heads,\n",
    "    d_ff = config.TimeLangNet_embedding_dim * 4,\n",
    "    d_k = config.TimeLangNet_d_k,\n",
    "    d_v = config.TimeLangNet_d_v,\n",
    "    hidden_dim= 64, \n",
    ").to(device)\n",
    "\n",
    "#  --optimizer RAdam\n",
    "# optimizer =torch.optim.AdamW(model.parameters(), lr=0.001,)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr=config.lr, \n",
    "                              betas=(0.9, 0.999),\n",
    "                              eps=1e-08, \n",
    "                              weight_decay=config.l2_weight)\n",
    "\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# optimizer = torch.optim.RAdam(model.parameters(), lr=config.lr)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size = 3, gamma=0.2)\n",
    "\n",
    "\n",
    "print(model)\n",
    "\n",
    "def get_parameter_number(model):\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return {'Total': total_num, 'Trainable': trainable_num}\n",
    "print(get_parameter_number(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d273d20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current lr : 1.2800000000000007e-08\n",
      "00:40:03 : Epoch: 1\n",
      "00:40:03 : Epoch [1/50],\n",
      "            Step [0],\n",
      "            Loss: 1.2186 \n",
      "00:40:09 : Epoch [1/50],\n",
      "            Step [100],\n",
      "            Loss: 1.2570 \n",
      "00:40:12 : Valid runing,Epoch [1/50],Loss: 0.8314\n",
      "00:40:12 : Valid runing,Epoch [1/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 2.5600000000000015e-09\n",
      "00:40:13 : Epoch: 2\n",
      "00:40:13 : Epoch [2/50],\n",
      "            Step [0],\n",
      "            Loss: 0.9397 \n",
      "00:40:18 : Epoch [2/50],\n",
      "            Step [100],\n",
      "            Loss: 1.1435 \n",
      "00:40:21 : Valid runing,Epoch [2/50],Loss: 0.8314\n",
      "00:40:22 : Valid runing,Epoch [2/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 2.5600000000000015e-09\n",
      "00:40:22 : Epoch: 3\n",
      "00:40:22 : Epoch [3/50],\n",
      "            Step [0],\n",
      "            Loss: 1.0984 \n",
      "00:40:27 : Epoch [3/50],\n",
      "            Step [100],\n",
      "            Loss: 1.2644 \n",
      "00:40:31 : Valid runing,Epoch [3/50],Loss: 0.8314\n",
      "00:40:31 : Valid runing,Epoch [3/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 2.5600000000000015e-09\n",
      "00:40:32 : Epoch: 4\n",
      "00:40:32 : Epoch [4/50],\n",
      "            Step [0],\n",
      "            Loss: 1.0040 \n",
      "00:40:38 : Epoch [4/50],\n",
      "            Step [100],\n",
      "            Loss: 1.5893 \n",
      "00:40:40 : Valid runing,Epoch [4/50],Loss: 0.8314\n",
      "00:40:41 : Valid runing,Epoch [4/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 5.120000000000003e-10\n",
      "00:40:42 : Epoch: 5\n",
      "00:40:42 : Epoch [5/50],\n",
      "            Step [0],\n",
      "            Loss: 1.3989 \n",
      "00:40:47 : Epoch [5/50],\n",
      "            Step [100],\n",
      "            Loss: 1.0883 \n",
      "00:40:50 : Valid runing,Epoch [5/50],Loss: 0.8314\n",
      "00:40:51 : Valid runing,Epoch [5/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 5.120000000000003e-10\n",
      "00:40:52 : Epoch: 6\n",
      "00:40:52 : Epoch [6/50],\n",
      "            Step [0],\n",
      "            Loss: 1.2964 \n",
      "00:40:58 : Epoch [6/50],\n",
      "            Step [100],\n",
      "            Loss: 1.4153 \n",
      "00:41:00 : Valid runing,Epoch [6/50],Loss: 0.8314\n",
      "00:41:00 : Valid runing,Epoch [6/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 5.120000000000003e-10\n",
      "00:41:01 : Epoch: 7\n",
      "00:41:01 : Epoch [7/50],\n",
      "            Step [0],\n",
      "            Loss: 0.6712 \n",
      "00:41:08 : Epoch [7/50],\n",
      "            Step [100],\n",
      "            Loss: 1.1076 \n",
      "00:41:09 : Valid runing,Epoch [7/50],Loss: 0.8314\n",
      "00:41:11 : Valid runing,Epoch [7/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 1.0240000000000007e-10\n",
      "00:41:11 : Epoch: 8\n",
      "00:41:11 : Epoch [8/50],\n",
      "            Step [0],\n",
      "            Loss: 1.1105 \n",
      "00:41:17 : Epoch [8/50],\n",
      "            Step [100],\n",
      "            Loss: 1.2354 \n",
      "00:41:20 : Valid runing,Epoch [8/50],Loss: 0.8314\n",
      "00:41:21 : Valid runing,Epoch [8/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 1.0240000000000007e-10\n",
      "00:41:21 : Epoch: 9\n",
      "00:41:21 : Epoch [9/50],\n",
      "            Step [0],\n",
      "            Loss: 0.7695 \n",
      "00:41:26 : Epoch [9/50],\n",
      "            Step [100],\n",
      "            Loss: 0.7755 \n",
      "00:41:29 : Valid runing,Epoch [9/50],Loss: 0.8314\n",
      "00:41:30 : Valid runing,Epoch [9/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 1.0240000000000007e-10\n",
      "00:41:30 : Epoch: 10\n",
      "00:41:30 : Epoch [10/50],\n",
      "            Step [0],\n",
      "            Loss: 1.1131 \n",
      "00:41:36 : Epoch [10/50],\n",
      "            Step [100],\n",
      "            Loss: 1.3554 \n",
      "00:41:39 : Valid runing,Epoch [10/50],Loss: 0.8314\n",
      "00:41:40 : Valid runing,Epoch [10/50],Loss: 1.5547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 2.0480000000000016e-11\n",
      "00:41:40 : Epoch: 11\n",
      "00:41:40 : Epoch [11/50],\n",
      "            Step [0],\n",
      "            Loss: 1.0376 \n",
      "00:41:46 : Epoch [11/50],\n",
      "            Step [100],\n",
      "            Loss: 0.8978 \n",
      "00:41:49 : Valid runing,Epoch [11/50],Loss: 0.8314\n",
      "00:41:49 : Valid runing,Epoch [11/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 2.0480000000000016e-11\n",
      "00:41:49 : Epoch: 12\n",
      "00:41:50 : Epoch [12/50],\n",
      "            Step [0],\n",
      "            Loss: 1.2879 \n",
      "00:41:55 : Epoch [12/50],\n",
      "            Step [100],\n",
      "            Loss: 1.4228 \n",
      "00:41:58 : Valid runing,Epoch [12/50],Loss: 0.8314\n",
      "00:41:59 : Valid runing,Epoch [12/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 2.0480000000000016e-11\n",
      "00:41:59 : Epoch: 13\n",
      "00:41:59 : Epoch [13/50],\n",
      "            Step [0],\n",
      "            Loss: 1.3977 \n",
      "00:42:05 : Epoch [13/50],\n",
      "            Step [100],\n",
      "            Loss: 1.1102 \n",
      "00:42:07 : Valid runing,Epoch [13/50],Loss: 0.8314\n",
      "00:42:09 : Valid runing,Epoch [13/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 4.0960000000000035e-12\n",
      "00:42:09 : Epoch: 14\n",
      "00:42:09 : Epoch [14/50],\n",
      "            Step [0],\n",
      "            Loss: 1.6576 \n",
      "00:42:14 : Epoch [14/50],\n",
      "            Step [100],\n",
      "            Loss: 1.2643 \n",
      "00:42:17 : Valid runing,Epoch [14/50],Loss: 0.8314\n",
      "00:42:18 : Valid runing,Epoch [14/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 4.0960000000000035e-12\n",
      "00:42:18 : Epoch: 15\n",
      "00:42:18 : Epoch [15/50],\n",
      "            Step [0],\n",
      "            Loss: 1.2334 \n",
      "00:42:24 : Epoch [15/50],\n",
      "            Step [100],\n",
      "            Loss: 1.1527 \n",
      "00:42:27 : Valid runing,Epoch [15/50],Loss: 0.8314\n",
      "00:42:27 : Valid runing,Epoch [15/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 4.0960000000000035e-12\n",
      "00:42:28 : Epoch: 16\n",
      "00:42:28 : Epoch [16/50],\n",
      "            Step [0],\n",
      "            Loss: 0.8858 \n",
      "00:42:34 : Epoch [16/50],\n",
      "            Step [100],\n",
      "            Loss: 1.1507 \n",
      "00:42:36 : Valid runing,Epoch [16/50],Loss: 0.8314\n",
      "00:42:37 : Valid runing,Epoch [16/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 8.192000000000008e-13\n",
      "00:42:37 : Epoch: 17\n",
      "00:42:38 : Epoch [17/50],\n",
      "            Step [0],\n",
      "            Loss: 1.2197 \n",
      "00:42:44 : Epoch [17/50],\n",
      "            Step [100],\n",
      "            Loss: 1.4462 \n",
      "00:42:46 : Valid runing,Epoch [17/50],Loss: 0.8314\n",
      "00:42:47 : Valid runing,Epoch [17/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 8.192000000000008e-13\n",
      "00:42:47 : Epoch: 18\n",
      "00:42:47 : Epoch [18/50],\n",
      "            Step [0],\n",
      "            Loss: 1.4161 \n",
      "00:42:54 : Epoch [18/50],\n",
      "            Step [100],\n",
      "            Loss: 1.0537 \n",
      "00:42:56 : Valid runing,Epoch [18/50],Loss: 0.8314\n",
      "00:42:56 : Valid runing,Epoch [18/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 8.192000000000008e-13\n",
      "00:42:57 : Epoch: 19\n",
      "00:42:57 : Epoch [19/50],\n",
      "            Step [0],\n",
      "            Loss: 1.2395 \n",
      "00:43:04 : Epoch [19/50],\n",
      "            Step [100],\n",
      "            Loss: 0.9047 \n",
      "00:43:05 : Valid runing,Epoch [19/50],Loss: 0.8314\n",
      "00:43:06 : Valid runing,Epoch [19/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 1.6384000000000016e-13\n",
      "00:43:07 : Epoch: 20\n",
      "00:43:07 : Epoch [20/50],\n",
      "            Step [0],\n",
      "            Loss: 1.2432 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:43:12 : Epoch [20/50],\n",
      "            Step [100],\n",
      "            Loss: 1.1550 \n",
      "00:43:14 : Valid runing,Epoch [20/50],Loss: 0.8314\n",
      "00:43:16 : Valid runing,Epoch [20/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 1.6384000000000016e-13\n",
      "00:43:16 : Epoch: 21\n",
      "00:43:16 : Epoch [21/50],\n",
      "            Step [0],\n",
      "            Loss: 1.4295 \n",
      "00:43:22 : Epoch [21/50],\n",
      "            Step [100],\n",
      "            Loss: 0.9931 \n",
      "00:43:23 : Valid runing,Epoch [21/50],Loss: 0.8314\n",
      "00:43:25 : Valid runing,Epoch [21/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 1.6384000000000016e-13\n",
      "00:43:25 : Epoch: 22\n",
      "00:43:25 : Epoch [22/50],\n",
      "            Step [0],\n",
      "            Loss: 1.3647 \n",
      "00:43:31 : Epoch [22/50],\n",
      "            Step [100],\n",
      "            Loss: 1.2215 \n",
      "00:43:33 : Valid runing,Epoch [22/50],Loss: 0.8314\n",
      "00:43:35 : Valid runing,Epoch [22/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 3.2768000000000034e-14\n",
      "00:43:35 : Epoch: 23\n",
      "00:43:35 : Epoch [23/50],\n",
      "            Step [0],\n",
      "            Loss: 1.1555 \n",
      "00:43:41 : Epoch [23/50],\n",
      "            Step [100],\n",
      "            Loss: 1.0684 \n",
      "00:43:42 : Valid runing,Epoch [23/50],Loss: 0.8314\n",
      "00:43:44 : Valid runing,Epoch [23/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 3.2768000000000034e-14\n",
      "00:43:45 : Epoch: 24\n",
      "00:43:45 : Epoch [24/50],\n",
      "            Step [0],\n",
      "            Loss: 1.3353 \n",
      "00:43:50 : Epoch [24/50],\n",
      "            Step [100],\n",
      "            Loss: 1.1438 \n",
      "00:43:51 : Valid runing,Epoch [24/50],Loss: 0.8314\n",
      "00:43:52 : Valid runing,Epoch [24/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 3.2768000000000034e-14\n",
      "00:43:52 : Epoch: 25\n",
      "00:43:53 : Epoch [25/50],\n",
      "            Step [0],\n",
      "            Loss: 1.0597 \n",
      "00:44:00 : Epoch [25/50],\n",
      "            Step [100],\n",
      "            Loss: 1.2064 \n",
      "00:44:01 : Valid runing,Epoch [25/50],Loss: 0.8314\n",
      "00:44:03 : Valid runing,Epoch [25/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 6.553600000000007e-15\n",
      "00:44:04 : Epoch: 26\n",
      "00:44:04 : Epoch [26/50],\n",
      "            Step [0],\n",
      "            Loss: 1.4439 \n",
      "00:44:10 : Epoch [26/50],\n",
      "            Step [100],\n",
      "            Loss: 1.4404 \n",
      "00:44:11 : Valid runing,Epoch [26/50],Loss: 0.8314\n",
      "00:44:13 : Valid runing,Epoch [26/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 6.553600000000007e-15\n",
      "00:44:14 : Epoch: 27\n",
      "00:44:14 : Epoch [27/50],\n",
      "            Step [0],\n",
      "            Loss: 1.1267 \n",
      "00:44:19 : Epoch [27/50],\n",
      "            Step [100],\n",
      "            Loss: 1.2523 \n",
      "00:44:22 : Valid runing,Epoch [27/50],Loss: 0.8314\n",
      "00:44:23 : Valid runing,Epoch [27/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 6.553600000000007e-15\n",
      "00:44:23 : Epoch: 28\n",
      "00:44:23 : Epoch [28/50],\n",
      "            Step [0],\n",
      "            Loss: 0.9632 \n",
      "00:44:29 : Epoch [28/50],\n",
      "            Step [100],\n",
      "            Loss: 1.2605 \n",
      "00:44:32 : Valid runing,Epoch [28/50],Loss: 0.8314\n",
      "00:44:33 : Valid runing,Epoch [28/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 1.3107200000000015e-15\n",
      "00:44:33 : Epoch: 29\n",
      "00:44:33 : Epoch [29/50],\n",
      "            Step [0],\n",
      "            Loss: 2.0350 \n",
      "00:44:39 : Epoch [29/50],\n",
      "            Step [100],\n",
      "            Loss: 0.8494 \n",
      "00:44:42 : Valid runing,Epoch [29/50],Loss: 0.8314\n",
      "00:44:42 : Valid runing,Epoch [29/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 1.3107200000000015e-15\n",
      "00:44:43 : Epoch: 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:44:43 : Epoch [30/50],\n",
      "            Step [0],\n",
      "            Loss: 0.9548 \n",
      "00:44:48 : Epoch [30/50],\n",
      "            Step [100],\n",
      "            Loss: 1.1728 \n",
      "00:44:51 : Valid runing,Epoch [30/50],Loss: 0.8314\n",
      "00:44:52 : Valid runing,Epoch [30/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 1.3107200000000015e-15\n",
      "00:44:52 : Epoch: 31\n",
      "00:44:53 : Epoch [31/50],\n",
      "            Step [0],\n",
      "            Loss: 1.0286 \n",
      "00:44:59 : Epoch [31/50],\n",
      "            Step [100],\n",
      "            Loss: 0.9534 \n",
      "00:45:01 : Valid runing,Epoch [31/50],Loss: 0.8314\n",
      "00:45:03 : Valid runing,Epoch [31/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 2.621440000000003e-16\n",
      "00:45:03 : Epoch: 32\n",
      "00:45:03 : Epoch [32/50],\n",
      "            Step [0],\n",
      "            Loss: 1.0191 \n",
      "00:45:09 : Epoch [32/50],\n",
      "            Step [100],\n",
      "            Loss: 1.3057 \n",
      "00:45:11 : Valid runing,Epoch [32/50],Loss: 0.8314\n",
      "00:45:13 : Valid runing,Epoch [32/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 2.621440000000003e-16\n",
      "00:45:13 : Epoch: 33\n",
      "00:45:13 : Epoch [33/50],\n",
      "            Step [0],\n",
      "            Loss: 1.3276 \n",
      "00:45:19 : Epoch [33/50],\n",
      "            Step [100],\n",
      "            Loss: 0.6704 \n",
      "00:45:21 : Valid runing,Epoch [33/50],Loss: 0.8314\n",
      "00:45:22 : Valid runing,Epoch [33/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 2.621440000000003e-16\n",
      "00:45:22 : Epoch: 34\n",
      "00:45:22 : Epoch [34/50],\n",
      "            Step [0],\n",
      "            Loss: 1.0104 \n",
      "00:45:30 : Epoch [34/50],\n",
      "            Step [100],\n",
      "            Loss: 1.2151 \n",
      "00:45:30 : Valid runing,Epoch [34/50],Loss: 0.8314\n",
      "00:45:32 : Valid runing,Epoch [34/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 5.2428800000000064e-17\n",
      "00:45:32 : Epoch: 35\n",
      "00:45:32 : Epoch [35/50],\n",
      "            Step [0],\n",
      "            Loss: 1.0926 \n",
      "00:45:39 : Epoch [35/50],\n",
      "            Step [100],\n",
      "            Loss: 1.2517 \n",
      "00:45:40 : Valid runing,Epoch [35/50],Loss: 0.8314\n",
      "00:45:41 : Valid runing,Epoch [35/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 5.2428800000000064e-17\n",
      "00:45:42 : Epoch: 36\n",
      "00:45:42 : Epoch [36/50],\n",
      "            Step [0],\n",
      "            Loss: 1.2640 \n",
      "00:45:49 : Epoch [36/50],\n",
      "            Step [100],\n",
      "            Loss: 1.0464 \n",
      "00:45:50 : Valid runing,Epoch [36/50],Loss: 0.8314\n",
      "00:45:51 : Valid runing,Epoch [36/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 5.2428800000000064e-17\n",
      "00:45:51 : Epoch: 37\n",
      "00:45:51 : Epoch [37/50],\n",
      "            Step [0],\n",
      "            Loss: 1.1798 \n",
      "00:45:59 : Epoch [37/50],\n",
      "            Step [100],\n",
      "            Loss: 1.2279 \n",
      "00:46:00 : Valid runing,Epoch [37/50],Loss: 0.8314\n",
      "00:46:02 : Valid runing,Epoch [37/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 1.0485760000000013e-17\n",
      "00:46:02 : Epoch: 38\n",
      "00:46:02 : Epoch [38/50],\n",
      "            Step [0],\n",
      "            Loss: 2.3931 \n",
      "00:46:08 : Epoch [38/50],\n",
      "            Step [100],\n",
      "            Loss: 1.2623 \n",
      "00:46:10 : Valid runing,Epoch [38/50],Loss: 0.8314\n",
      "00:46:12 : Valid runing,Epoch [38/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 1.0485760000000013e-17\n",
      "00:46:12 : Epoch: 39\n",
      "00:46:13 : Epoch [39/50],\n",
      "            Step [0],\n",
      "            Loss: 1.0660 \n",
      "00:46:18 : Epoch [39/50],\n",
      "            Step [100],\n",
      "            Loss: 0.8188 \n",
      "00:46:20 : Valid runing,Epoch [39/50],Loss: 0.8314\n",
      "00:46:21 : Valid runing,Epoch [39/50],Loss: 1.5547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 1.0485760000000013e-17\n",
      "00:46:21 : Epoch: 40\n",
      "00:46:21 : Epoch [40/50],\n",
      "            Step [0],\n",
      "            Loss: 1.1492 \n",
      "00:46:28 : Epoch [40/50],\n",
      "            Step [100],\n",
      "            Loss: 1.4639 \n",
      "00:46:29 : Valid runing,Epoch [40/50],Loss: 0.8314\n",
      "00:46:30 : Valid runing,Epoch [40/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 2.097152000000003e-18\n",
      "00:46:31 : Epoch: 41\n",
      "00:46:31 : Epoch [41/50],\n",
      "            Step [0],\n",
      "            Loss: 0.9372 \n",
      "00:46:37 : Epoch [41/50],\n",
      "            Step [100],\n",
      "            Loss: 1.0803 \n",
      "00:46:39 : Valid runing,Epoch [41/50],Loss: 0.8314\n",
      "00:46:40 : Valid runing,Epoch [41/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 2.097152000000003e-18\n",
      "00:46:41 : Epoch: 42\n",
      "00:46:41 : Epoch [42/50],\n",
      "            Step [0],\n",
      "            Loss: 1.2479 \n",
      "00:46:48 : Epoch [42/50],\n",
      "            Step [100],\n",
      "            Loss: 0.8002 \n",
      "00:46:49 : Valid runing,Epoch [42/50],Loss: 0.8314\n",
      "00:46:50 : Valid runing,Epoch [42/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 2.097152000000003e-18\n",
      "00:46:50 : Epoch: 43\n",
      "00:46:50 : Epoch [43/50],\n",
      "            Step [0],\n",
      "            Loss: 1.1368 \n",
      "00:46:57 : Epoch [43/50],\n",
      "            Step [100],\n",
      "            Loss: 1.1425 \n",
      "00:46:59 : Valid runing,Epoch [43/50],Loss: 0.8314\n",
      "00:46:59 : Valid runing,Epoch [43/50],Loss: 1.5547\n",
      "Accuracy: 0.3231939163498099\n",
      "Precision: 0.32210453480945284\n",
      "Recall: 0.322027972027972\n",
      "F1 Score: 0.27256158429489313\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.64      0.45        66\n",
      "           1       0.57      0.18      0.28        66\n",
      "           2       0.27      0.45      0.34        66\n",
      "           3       0.10      0.02      0.03        65\n",
      "\n",
      "    accuracy                           0.32       263\n",
      "   macro avg       0.32      0.32      0.27       263\n",
      "weighted avg       0.32      0.32      0.27       263\n",
      "\n",
      "Current lr : 4.194304000000006e-19\n",
      "00:47:00 : Epoch: 44\n",
      "00:47:00 : Epoch [44/50],\n",
      "            Step [0],\n",
      "            Loss: 1.3435 \n",
      "00:47:07 : Epoch [44/50],\n",
      "            Step [100],\n",
      "            Loss: 1.2404 \n",
      "00:47:08 : Valid runing,Epoch [44/50],Loss: 0.8314\n"
     ]
    }
   ],
   "source": [
    "early_stop_patience = 10 \n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "import math\n",
    "import os\n",
    "import numpy\n",
    "import time, json, datetime \n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, recall_score, f1_score, precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "regression_criterion = torch.nn.MSELoss()\n",
    "\n",
    "def write_log(w):\n",
    "#     file_name = 'logs/' + datetime.date.today().strftime('%m%d')+\"_{}.log\".format(\"deepfm\")\n",
    "    t0 = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "    info = \"{} : {}\".format(t0, w)\n",
    "    print(info)\n",
    "    \n",
    "best_ks = 0.0\n",
    "best_epoch = 0\n",
    "num_epochs = config.epoch\n",
    "\n",
    "def cal_accuracy(y_pred, y_true):\n",
    "    return np.mean(y_pred == y_true)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_tmp = model.train()\n",
    "    train_loss = 0.0\n",
    "    train_cnt = 0\n",
    "    print(\"Current lr : {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    write_log('Epoch: {}'.format(epoch + 1))\n",
    "        \n",
    "    for step,(features , labels, lengths) in enumerate(train_dataloader):\n",
    "        ts_feature = features\n",
    "        label = labels\n",
    "        len_ts= lengths\n",
    "        train_loss = []\n",
    "        ts_feature = ts_feature.to(device)\n",
    "       \n",
    "        label = label.long().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ts_feature, \n",
    "                len_ts)\n",
    "#         print(outputs)\n",
    "        loss = criterion(outputs, label)\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        if step % 100 == 0:\n",
    "            msg = f\"\"\"Epoch [{epoch+1}/{num_epochs}],\n",
    "            Step [{step}],\n",
    "            Loss: {loss.item():.4f} \"\"\"\n",
    "            write_log(msg)\n",
    "        train_cnt += 1\n",
    "#         break\n",
    "        \n",
    "    scheduler.step()\n",
    "#     train_loss /= train_cnt\n",
    "#     print('val is begin')\n",
    "    val_tmp = model.eval()\n",
    "    val_loss = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    val_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for val_step,(features,labels, lengths) in enumerate(test_dataloader):\n",
    "            ts_feature= features\n",
    "            label = labels\n",
    "            len_ts = lengths\n",
    "            \n",
    "            label = label.long().to(device)\n",
    "            ts_feature = ts_feature.to(device)\n",
    "         \n",
    "            outputs = model(ts_feature,\n",
    "                len_ts)\n",
    "                        \n",
    "            loss = criterion(outputs, label)\n",
    "            val_loss.append(loss)\n",
    "            \n",
    "            \n",
    "            all_preds.append(outputs.detach())\n",
    "            all_labels.append(label)\n",
    "                \n",
    "            if val_step % 100 == 0:\n",
    "                msg = f'Valid runing,Epoch [{epoch+1}/{num_epochs}],Loss: {loss.item():.4f}'\n",
    "            \n",
    "            val_cnt += 1\n",
    "#             break\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    probs = torch.nn.functional.softmax(all_preds)  # (total_samples, num_classes) est. prob. for each class and sample\n",
    "    all_preds = torch.argmax(probs, dim=1).cpu().numpy()  # (total_samples,) int class index for each sample\n",
    "    all_labels = all_labels.flatten().cpu().numpy()\n",
    "#     accuracy = cal_accuracy(predictions, trues)\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    class_report = classification_report(all_labels, all_preds)\n",
    "\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print('Classification Report\\n', class_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c7fed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
